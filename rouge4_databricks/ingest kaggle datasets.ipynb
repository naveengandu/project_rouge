{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d626a8b-863b-46c1-9fa5-c09177c0c5cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2784caab-f9b0-4ba5-89fa-5c484ec3ecc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.list(scope = \"rouge4kv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe27229-453a-4d68-ad00-fe472c20721f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd5afb8-fb15-4540-ae24-d2bc987382cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import necessary modules and libraries\n",
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb96814-1c4c-4adf-b395-e78a9500e0e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Assign secrets to variables\n",
    "\n",
    "#Kaggle\n",
    "kaggle_username = dbutils.secrets.get(scope = \"rouge4kv1\", key = \"kaggleusername\")\n",
    "kaggle_key = dbutils.secrets.get(scope = \"rouge4kv1\", key = \"kagglekey\")\n",
    "\n",
    "#Azure Data Lake Storage\n",
    "STORAGE_ACCOUNT_NAME = \"rougestorageacc1\"\n",
    "STORAGE_ACCOUNT_KEY = dbutils.secrets.get(scope = \"rouge4kv1\", key = \"rougestorageacc1key1\")\n",
    "CONTAINER_NAME = \"rawkaggledata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1fb28a4-0c47-4a28-901b-5e58c79a8b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assign the authentication details prior to importing the Kaggle library because it authenticates at the time of import.\n",
    "os.environ['KAGGLE_USERNAME'] = kaggle_username\n",
    "os.environ['KAGGLE_KEY'] = kaggle_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8fbabb3-1f0e-4497-88d1-7c2290cbcca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the kaggle library\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21f2873-dcbe-462b-95e3-8f73ef9764ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04c442e-d1d0-4b4f-aff3-da3aa23fd1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Azure Blob Storage (ADLS Gen2)\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\",\n",
    "                                        credential=STORAGE_ACCOUNT_KEY)\n",
    "container_client = blob_service_client.get_container_client(CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28162f41-d424-4667-b727-53c46db6af13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "\n",
    "# - Note to self - probably best to arrange the list in order of increasing dataset size and parallelize. This is \n",
    "# executed on a single node but might be more cost effective and efficient to run this on multiple nodes due to the huge \n",
    "# dataset sizes and the nested for loops. \n",
    "\n",
    "# - The job crashed the first time I ran it, so I rearranged the order of the datasets. \n",
    "\n",
    "# - The dataset (\"minhhuy2810/rice-diseases-image-dataset\") is too large to download in one go and so running it always \n",
    "# displays an error - \"Fatal error: The Python kernel is unresponsive.\". So removing it from the list of datasets to be \n",
    "# downloaded. A more powerful cluster would be required to download the entire dataset. \n",
    "\n",
    "datasets = [\"vbookshelf/rice-leaf-diseases\", \"maimunulkjisan/rice-leaf-dataset-from-mendeley-data\", \"anshulm257/rice-disease-dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13437c3-cc9a-42cc-9598-71e31d35fd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Download the entire dataset to the temporary directory\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Create a temporary directory in Databricks\n",
    "    temp_dir = \"/dbfs/tmp/kaggle_datasets/\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    api.dataset_download_files(dataset, path=temp_dir, unzip=True)\n",
    "\n",
    "    # Upload each file to Azure Data Lake\n",
    "    for root, _, files in os.walk(temp_dir):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            blob_path = os.path.relpath(file_path, temp_dir)  # Preserve folder structure\n",
    "\n",
    "            print(f\"Uploading {blob_path} to Azure Data Lake...\")\n",
    "\n",
    "            # Read file in binary mode\n",
    "            with open(file_path, \"rb\") as data:\n",
    "                blob_client = container_client.get_blob_client(blob_path)\n",
    "                blob_client.upload_blob(data, overwrite=True)\n",
    "            \n",
    "    # Clean up local files\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"{dataset} uploaded successfully and temporary files deleted.\")\n",
    "\n",
    "print(\"All datasets uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6dd519c-5661-485d-89b0-22b0bb331023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# verify that the dataset is available in the specified storage account/container.\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    f\"{STORAGE_ACCOUNT_KEY}\"\n",
    ")\n",
    "\n",
    "display(dbutils.fs.ls(f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest kaggle datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
