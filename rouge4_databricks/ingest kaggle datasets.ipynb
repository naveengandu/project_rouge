{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe27229-453a-4d68-ad00-fe472c20721f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install kaggle azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dd5afb8-fb15-4540-ae24-d2bc987382cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1fb28a4-0c47-4a28-901b-5e58c79a8b28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Assign the authentication details prior to importing the Kaggle library because it authenticates at the time of import.\n",
    "os.environ['KAGGLE_USERNAME'] = 'naveengandu'\n",
    "os.environ['KAGGLE_KEY'] = '407179e1a8def05b1b8baf22198b993a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8fbabb3-1f0e-4497-88d1-7c2290cbcca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the kaggle library\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c21f2873-dcbe-462b-95e3-8f73ef9764ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "125a4472-7e1b-4c48-ab7e-44162e087edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Azure Data Lake Storage credentials\n",
    "# Hardcoding these keys isn't the best practice due to potential security risks. However, for the sake of simplicity and limitations of the databricks free trial subscription, this method was followed.\n",
    "STORAGE_ACCOUNT_NAME = \"projectrouge6363905707\"\n",
    "STORAGE_ACCOUNT_KEY = \"Mrth7ayFyx5DPRYkTmtMwRFrq0IU9WQuB/xCFsNbn/ApJOTP1m66DM5/M8BSikzUPMVTmxPcIRgc+AStAPeDdg==\"\n",
    "CONTAINER_NAME = \"rawkaggledata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04c442e-d1d0-4b4f-aff3-da3aa23fd1a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Connect to Azure Blob Storage (ADLS Gen2)\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\",\n",
    "                                        credential=STORAGE_ACCOUNT_KEY)\n",
    "container_client = blob_service_client.get_container_client(CONTAINER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28162f41-d424-4667-b727-53c46db6af13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "\n",
    "# - Note to self - probably best to arrange the list in order of increasing dataset size and parallelize. This is \n",
    "# executed on a single node but might be more cost effective and efficient to run this on multiple nodes due to the huge \n",
    "# dataset sizes and the nested for loops. \n",
    "\n",
    "# - The job crashed the first time I ran it, so I rearranged the order of the datasets. \n",
    "\n",
    "# - The dataset (\"minhhuy2810/rice-diseases-image-dataset\") is too large to download in one go and so running it always \n",
    "# displays an error - \"Fatal error: The Python kernel is unresponsive.\". So removing it from the list of datasets to be \n",
    "# downloaded. A more powerful cluster would be required to download the entire dataset. \n",
    "\n",
    "datasets = [\"vbookshelf/rice-leaf-diseases\", \"maimunulkjisan/rice-leaf-dataset-from-mendeley-data\", \"anshulm257/rice-disease-dataset\", \"minhhuy2810/rice-diseases-image-dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13437c3-cc9a-42cc-9598-71e31d35fd85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Download the entire dataset to the temporary directory\n",
    "print(\"Downloading dataset from Kaggle...\")\n",
    "\n",
    "for dataset in datasets:\n",
    "    # Create a temporary directory in Databricks\n",
    "    temp_dir = \"/dbfs/tmp/kaggle_datasets/\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    api.dataset_download_files(dataset, path=temp_dir, unzip=True)\n",
    "\n",
    "    # Upload each file to Azure Data Lake\n",
    "    for root, _, files in os.walk(temp_dir):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            blob_path = os.path.relpath(file_path, temp_dir)  # Preserve folder structure\n",
    "\n",
    "            print(f\"Uploading {blob_path} to Azure Data Lake...\")\n",
    "\n",
    "            # Read file in binary mode\n",
    "            with open(file_path, \"rb\") as data:\n",
    "                blob_client = container_client.get_blob_client(blob_path)\n",
    "                blob_client.upload_blob(data, overwrite=True)\n",
    "            \n",
    "    # Clean up local files\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"{dataset} uploaded successfully and temporary files deleted.\")\n",
    "\n",
    "print(\"All datasets uploaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a0a7025-b6e7-44a7-b3d2-aed854a6a7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clean up local files\n",
    "import shutil\n",
    "\n",
    "temp_dir = \"/dbfs/tmp/kaggle_datasets/\"\n",
    "shutil.rmtree(temp_dir)\n",
    "print(\"âœ… All datasets uploaded successfully and temporary files deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6dd519c-5661-485d-89b0-22b0bb331023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# verify that the dataset is available in the specified storage account/container.\n",
    "spark.conf.set(\n",
    "    f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\",\n",
    "    f\"{STORAGE_ACCOUNT_KEY}\"\n",
    ")\n",
    "\n",
    "display(dbutils.fs.ls(f\"abfss://{CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest kaggle datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
